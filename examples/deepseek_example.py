#!/usr/bin/env python3
"""
DeepSeek API ä½¿ç”¨ç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•åœ¨ LOOM ä¸­ä½¿ç”¨ DeepSeek ä½œä¸º LLM æä¾›å•†ï¼Œ
åŒ…æ‹¬ä¸­æ–‡å†…å®¹ç”Ÿæˆã€æ¨ç†æ¨¡å¼ã€æˆæœ¬è®¡ç®—ç­‰åŠŸèƒ½ã€‚

ä½¿ç”¨æ–¹æ³•:
1. è®¾ç½®ç¯å¢ƒå˜é‡: export DEEPSEEK_API_KEY="your-api-key"
2. è¿è¡Œ: python deepseek_example.py
"""

import asyncio
import os
import sys
from datetime import datetime
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from loom.interpretation import LLMProviderFactory
    from loom import SessionManager, SessionConfig
except ImportError:
    print("é”™è¯¯: æ— æ³•å¯¼å…¥ LOOM æ¨¡å—ã€‚è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬ã€‚")
    print("å°è¯•: cd /path/to/ai-loom && python examples/deepseek_example.py")
    sys.exit(1)


class DeepSeekExample:
    """DeepSeek ä½¿ç”¨ç¤ºä¾‹ç±»"""

    def __init__(self, api_key: str = None):
        """
        åˆå§‹åŒ–ç¤ºä¾‹

        Args:
            api_key: DeepSeek API å¯†é’¥ï¼Œå¦‚æœä¸º None åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        """
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        if not self.api_key:
            print("é”™è¯¯: æœªæ‰¾åˆ° DeepSeek API å¯†é’¥")
            print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export DEEPSEEK_API_KEY='your-api-key'")
            print("æˆ–åœ¨ä»£ç ä¸­ç›´æ¥æä¾› API å¯†é’¥")
            sys.exit(1)

    def create_deepseek_config(self, thinking_enabled: bool = False) -> Dict[str, Any]:
        """
        åˆ›å»º DeepSeek é…ç½®

        Args:
            thinking_enabled: æ˜¯å¦å¯ç”¨æ¨ç†æ¨¡å¼

        Returns:
            DeepSeek é…ç½®å­—å…¸
        """
        model = "deepseek-reasoner" if thinking_enabled else "deepseek-chat"
        max_tokens = 32000 if thinking_enabled else 4096

        return {
            "type": "deepseek",
            "api_key": self.api_key,
            "base_url": "https://api.deepseek.com",
            "model": model,
            "thinking_enabled": thinking_enabled,
            "temperature": 1.0,
            "max_tokens": max_tokens,
            "timeout": 60,
            "max_retries": 3,
            "retry_delay": 2.0,
            "enable_caching": True,
            "cache_ttl": 300,
            "connection_pool_size": 5,
        }

    async def test_connection(self):
        """æµ‹è¯• DeepSeek è¿æ¥"""
        print("=" * 60)
        print("æµ‹è¯• DeepSeek è¿æ¥")
        print("=" * 60)

        config = self.create_deepseek_config()
        provider = LLMProviderFactory.create_provider(config)

        try:
            # ç®€å•æµ‹è¯•è¯·æ±‚
            response = await provider.generate("è¯·å›å¤'è¿æ¥æµ‹è¯•æˆåŠŸ'")

            print(f"âœ“ è¿æ¥æµ‹è¯•æˆåŠŸ")
            print(f"  æ¨¡å‹: {response.model}")
            print(f"  å“åº”: {response.content}")
            print(f"  ä»¤ç‰Œä½¿ç”¨: {response.usage}")
            print(f"  æˆæœ¬: ${response.cost:.6f}")

            return True

        except Exception as e:
            print(f"âœ— è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False

    async def chinese_content_generation(self):
        """ä¸­æ–‡å†…å®¹ç”Ÿæˆç¤ºä¾‹"""
        print("\n" + "=" * 60)
        print("ä¸­æ–‡å†…å®¹ç”Ÿæˆç¤ºä¾‹")
        print("=" * 60)

        config = self.create_deepseek_config()
        provider = LLMProviderFactory.create_provider(config)

        # ä¸­æ–‡å†…å®¹ç”Ÿæˆä»»åŠ¡
        tasks = [
            {"name": "è¯—æ­Œåˆ›ä½œ", "prompt": "åˆ›ä½œä¸€é¦–å…³äºä¸­ç§‹èŠ‚çš„ä¸ƒè¨€ç»å¥"},
            {
                "name": "æ•…äº‹åˆ›ä½œ",
                "prompt": "å†™ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½åŠ©æ‰‹çš„ç§‘å¹»å¾®å°è¯´ï¼Œä¸è¶…è¿‡200å­—",
            },
            {
                "name": "æ–‡ç« æ‘˜è¦",
                "prompt": """è¯·ä¸ºä»¥ä¸‹æ–‡ç« å†™ä¸€ä¸ªä¸­æ–‡æ‘˜è¦ï¼š
                äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚ä»æ™ºèƒ½æ‰‹æœºåŠ©æ‰‹åˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œ
                ä»åŒ»ç–—è¯Šæ–­åˆ°é‡‘èåˆ†æï¼ŒAIæŠ€æœ¯å·²ç»æ¸—é€åˆ°å„ä¸ªé¢†åŸŸã€‚
                æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œäººå·¥æ™ºèƒ½å°†åœ¨æ•™è‚²ã€å¨±ä¹ã€å·¥ä½œç­‰æ–¹é¢å‘æŒ¥æ›´å¤§ä½œç”¨ã€‚
                ç„¶è€Œï¼Œè¿™ä¹Ÿå¸¦æ¥äº†ä¼¦ç†ã€éšç§å’Œå°±ä¸šç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬éœ€è¦åœ¨äº«å—æŠ€æœ¯å¸¦æ¥çš„ä¾¿åˆ©çš„åŒæ—¶ï¼Œ
                è®¤çœŸæ€è€ƒå¦‚ä½•åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚""",
            },
        ]

        results = []
        total_cost = 0

        for task in tasks:
            print(f"\nâ–¶ {task['name']}")
            print(f"  æç¤º: {task['prompt'][:50]}...")

            try:
                response = await provider.generate(
                    task["prompt"], temperature=0.8, max_tokens=500
                )

                print(f"  âœ“ ç”ŸæˆæˆåŠŸ")
                print(f"    å“åº”: {response.content[:80]}...")
                print(f"    é•¿åº¦: {len(response.content)} å­—ç¬¦")
                print(f"    ä»¤ç‰Œ: {response.usage.get('total_tokens', 0)}")
                print(f"    æˆæœ¬: ${response.cost:.6f}")

                results.append(
                    {
                        "task": task["name"],
                        "content": response.content,
                        "tokens": response.usage.get("total_tokens", 0),
                        "cost": response.cost,
                    }
                )

                total_cost += response.cost

            except Exception as e:
                print(f"  âœ— ç”Ÿæˆå¤±è´¥: {e}")
                results.append({"task": task["name"], "error": str(e)})

        print(f"\nğŸ“Š ä¸­æ–‡å†…å®¹ç”Ÿæˆç»Ÿè®¡:")
        print(f"  æ€»ä»»åŠ¡æ•°: {len(tasks)}")
        print(f"  æˆåŠŸæ•°: {len([r for r in results if 'content' in r])}")
        print(f"  æ€»æˆæœ¬: ${total_cost:.6f}")

        return results

    async def reasoning_mode_demo(self):
        """æ¨ç†æ¨¡å¼æ¼”ç¤º"""
        print("\n" + "=" * 60)
        print("æ¨ç†æ¨¡å¼æ¼”ç¤º")
        print("=" * 60)

        config = self.create_deepseek_config(thinking_enabled=True)
        provider = LLMProviderFactory.create_provider(config)

        # æ¨ç†é—®é¢˜
        reasoning_problems = [
            {
                "category": "é€»è¾‘æ¨ç†",
                "problem": "å¦‚æœæ‰€æœ‰çŒ«éƒ½æ€•æ°´ï¼Œè€Œæ±¤å§†æ˜¯ä¸€åªçŒ«ï¼Œé‚£ä¹ˆæ±¤å§†æ€•æ°´å—ï¼Ÿè¯·å±•ç¤ºå®Œæ•´çš„æ¨ç†è¿‡ç¨‹ã€‚",
            },
            {
                "category": "æ•°å­¦é—®é¢˜",
                "problem": """ä¸€ä¸ªæ°´æ± æœ‰è¿›æ°´ç®¡å’Œå‡ºæ°´ç®¡ã€‚è¿›æ°´ç®¡å•ç‹¬æ³¨æ»¡æ°´æ± éœ€è¦3å°æ—¶ï¼Œ
                å‡ºæ°´ç®¡å•ç‹¬æ’ç©ºæ°´æ± éœ€è¦4å°æ—¶ã€‚å¦‚æœåŒæ—¶æ‰“å¼€è¿›æ°´ç®¡å’Œå‡ºæ°´ç®¡ï¼Œéœ€è¦å¤šå°‘å°æ—¶æ‰èƒ½æ³¨æ»¡æ°´æ± ï¼Ÿ
                è¯·å±•ç¤ºè®¡ç®—æ­¥éª¤ã€‚""",
            },
            {
                "category": "ç§‘å­¦æ¨ç†",
                "problem": "è§£é‡Šä¸ºä»€ä¹ˆå†°ä¼šæµ®åœ¨æ°´é¢ä¸Šï¼Œè€Œå¤§å¤šæ•°å›ºä½“éƒ½ä¼šä¸‹æ²‰ã€‚è¯·ä»ç‰©ç†åŸç†è§’åº¦è§£é‡Šã€‚",
            },
        ]

        results = []

        for problem in reasoning_problems:
            print(f"\nâ–¶ {problem['category']}")
            print(f"  é—®é¢˜: {problem['problem']}")

            try:
                response = await provider.generate(
                    problem["problem"],
                    temperature=0.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„æ¨ç†
                    max_tokens=1000,
                )

                print(f"  âœ“ æ¨ç†å®Œæˆ")
                print(f"    å“åº”æ‘˜è¦: {response.content[:100]}...")
                print(f"    æ¨¡å‹: {response.model}")
                print(
                    f"    æ¨ç†æ¨¡å¼: {response.metadata.get('thinking_enabled', False)}"
                )
                print(f"    ä»¤ç‰Œ: {response.usage.get('total_tokens', 0)}")
                print(f"    æˆæœ¬: ${response.cost:.6f}")

                # ä¿å­˜å®Œæ•´å“åº”åˆ°æ–‡ä»¶
                filename = f"reasoning_{problem['category']}.txt"
                with open(filename, "w", encoding="utf-8") as f:
                    f.write(f"é—®é¢˜: {problem['problem']}\n\n")
                    f.write(f"å›ç­”:\n{response.content}\n\n")
                    f.write(f"å…ƒæ•°æ®: {response.metadata}\n")
                    f.write(f"ä»¤ç‰Œä½¿ç”¨: {response.usage}\n")
                    f.write(f"æˆæœ¬: ${response.cost:.6f}\n")

                print(f"    å®Œæ•´å“åº”å·²ä¿å­˜åˆ°: {filename}")

                results.append(
                    {
                        "category": problem["category"],
                        "response": response.content,
                        "tokens": response.usage.get("total_tokens", 0),
                        "cost": response.cost,
                        "file": filename,
                    }
                )

            except Exception as e:
                print(f"  âœ— æ¨ç†å¤±è´¥: {e}")
                results.append({"category": problem["category"], "error": str(e)})

        return results

    async def session_with_deepseek(self):
        """ä½¿ç”¨ DeepSeek çš„å®Œæ•´ä¼šè¯ç¤ºä¾‹"""
        print("\n" + "=" * 60)
        print("å®Œæ•´ä¼šè¯ç¤ºä¾‹")
        print("=" * 60)

        # åˆ›å»ºä¼šè¯ç®¡ç†å™¨
        session_manager = SessionManager()

        # é…ç½®ä½¿ç”¨ DeepSeek
        session_config = SessionConfig(
            session_type="chinese_content",
            initial_prompt="æˆ‘ä»¬æ¥è®¨è®ºä¸­å›½ä¼ ç»Ÿæ–‡åŒ–",
            llm_provider="deepseek",
            llm_model="deepseek-chat",
            max_turns=3,
            memory_enabled=True,
        )

        print("åˆ›å»ºä¼šè¯...")
        session = await session_manager.create_session(session_config)
        print(f"ä¼šè¯ID: {session.session_id}")

        # å¯¹è¯æµç¨‹
        conversation = [
            "é¦–å…ˆï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä¸­å›½ä¼ ç»ŸèŠ‚æ—¥",
            "é‚£ä¹ˆåœ¨è¿™äº›èŠ‚æ—¥ä¸­ï¼Œé£Ÿç‰©æœ‰ä»€ä¹ˆç‰¹åˆ«çš„å«ä¹‰å—ï¼Ÿ",
            "æœ€åï¼Œç°ä»£ç¤¾ä¼šä¸­è¿™äº›ä¼ ç»ŸèŠ‚æ—¥æœ‰ä»€ä¹ˆæ–°çš„å˜åŒ–ï¼Ÿ",
        ]

        print("\nå¼€å§‹å¯¹è¯:")

        for i, user_input in enumerate(conversation, 1):
            print(f"\n[ç”¨æˆ·] ç¬¬{i}è½®: {user_input}")

            try:
                # å‘é€ç”¨æˆ·è¾“å…¥
                turn = await session.add_turn(user_input)

                print(f"[AI] å“åº”: {turn.response[:150]}...")
                print(f"    æ¨¡å‹: {turn.model}")
                print(f"    ä»¤ç‰Œ: {turn.usage.get('total_tokens', 0)}")
                print(f"    æˆæœ¬: ${turn.cost:.6f}")

                # æ¨¡æ‹Ÿç”¨æˆ·æ€è€ƒæ—¶é—´
                await asyncio.sleep(1)

            except Exception as e:
                print(f"  âœ— å¯¹è¯å¤±è´¥: {e}")
                break

        print(f"\nğŸ“Š ä¼šè¯ç»Ÿè®¡:")
        print(f"  æ€»å›åˆæ•°: {len(session.turns)}")
        print(f"  æ€»æˆæœ¬: ${sum(t.cost for t in session.turns):.6f}")

        # ä¿å­˜ä¼šè¯è®°å½•
        filename = f"session_{session.session_id}.txt"
        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"ä¼šè¯ID: {session.session_id}\n")
            f.write(f"åˆ›å»ºæ—¶é—´: {datetime.now().isoformat()}\n")
            f.write(f"ä¼šè¯ç±»å‹: {session_config.session_type}\n")
            f.write(f"LLMæä¾›å•†: {session_config.llm_provider}\n")
            f.write(f"LLMæ¨¡å‹: {session_config.llm_model}\n")
            f.write("\n" + "=" * 50 + "\n\n")

            for i, turn in enumerate(session.turns, 1):
                f.write(f"å›åˆ {i}:\n")
                f.write(f"ç”¨æˆ·: {turn.prompt}\n")
                f.write(f"AI: {turn.response}\n")
                f.write(f"æ¨¡å‹: {turn.model}\n")
                f.write(f"ä»¤ç‰Œ: {turn.usage}\n")
                f.write(f"æˆæœ¬: ${turn.cost:.6f}\n")
                f.write("\n" + "-" * 30 + "\n\n")

        print(f"ä¼šè¯è®°å½•å·²ä¿å­˜åˆ°: {filename}")

        return session

    async def cost_comparison(self):
        """æˆæœ¬æ¯”è¾ƒç¤ºä¾‹"""
        print("\n" + "=" * 60)
        print("æˆæœ¬æ¯”è¾ƒç¤ºä¾‹")
        print("=" * 60)

        # æµ‹è¯•æ–‡æœ¬
        test_prompts = [
            "å†™ä¸€å¥ç®€å•çš„é—®å€™è¯­",
            "å†™ä¸€æ®µäº§å“æè¿°ï¼Œå¤§çº¦100å­—",
            "å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„çŸ­æ–‡ï¼Œä¸å°‘äº300å­—",
        ]

        # é…ç½®ä¸åŒæä¾›å•†
        providers_config = {
            "deepseek": self.create_deepseek_config(),
            "openai": {
                "type": "openai",
                "api_key": os.getenv("OPENAI_API_KEY", "test-key"),
                "model": "gpt-3.5-turbo",
                "temperature": 0.7,
                "max_tokens": 1000,
            },
        }

        results = {}

        for provider_name, config in providers_config.items():
            print(f"\næµ‹è¯• {provider_name}...")

            # è·³è¿‡æ²¡æœ‰APIå¯†é’¥çš„æä¾›å•†
            if provider_name == "openai" and not os.getenv("OPENAI_API_KEY"):
                print("  è·³è¿‡: æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
                continue

            provider_results = []
            total_cost = 0

            for prompt in test_prompts:
                try:
                    provider = LLMProviderFactory.create_provider(config)
                    response = await provider.generate(prompt)

                    provider_results.append(
                        {
                            "prompt": prompt[:30] + "...",
                            "tokens": response.usage.get("total_tokens", 0),
                            "cost": response.cost,
                        }
                    )

                    total_cost += response.cost

                except Exception as e:
                    print(f"  âœ— æµ‹è¯•å¤±è´¥: {e}")
                    provider_results.append(
                        {"prompt": prompt[:30] + "...", "error": str(e)}
                    )

            results[provider_name] = {
                "results": provider_results,
                "total_cost": total_cost,
            }

        # è¾“å‡ºæ¯”è¾ƒç»“æœ
        print("\nğŸ“Š æˆæœ¬æ¯”è¾ƒç»“æœ:")
        for provider_name, data in results.items():
            print(f"\n{provider_name}:")
            if "results" in data:
                for result in data["results"]:
                    if "cost" in result:
                        print(f"  {result['prompt']}")
                        print(f"    ä»¤ç‰Œ: {result['tokens']}")
                        print(f"    æˆæœ¬: ${result['cost']:.6f}")
                print(f"  æ€»æˆæœ¬: ${data['total_cost']:.6f}")

        return results

    async def run_all_examples(self):
        """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
        print("DeepSeek API ä½¿ç”¨ç¤ºä¾‹")
        print("=" * 60)
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().isoformat()}")
        print(
            f"APIå¯†é’¥: {self.api_key[:10]}...{self.api_key[-4:] if len(self.api_key) > 14 else ''}"
        )
        print()

        # è¿è¡Œå„ä¸ªç¤ºä¾‹
        await self.test_connection()
        await self.chinese_content_generation()
        await self.reasoning_mode_demo()
        await self.session_with_deepseek()
        await self.cost_comparison()

        print("\n" + "=" * 60)
        print("æ‰€æœ‰ç¤ºä¾‹å®Œæˆ!")
        print(f"ç»“æŸæ—¶é—´: {datetime.now().isoformat()}")
        print("=" * 60)


async def main():
    """ä¸»å‡½æ•°"""
    # ä»ç¯å¢ƒå˜é‡æˆ–å‘½ä»¤è¡Œå‚æ•°è·å–APIå¯†é’¥
    api_key = None
    if len(sys.argv) > 1:
        api_key = sys.argv[1]

    example = DeepSeekExample(api_key)

    try:
        await example.run_all_examples()
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\né”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
