#!/usr/bin/env python3
"""
LLM Providerå¢å¼ºç»„ä»¶é›†æˆéªŒè¯è„šæœ¬

éªŒè¯EnhancedProviderManagerã€CostOptimizerå’ŒLocalModelProviderä¸ç°æœ‰ç³»ç»Ÿçš„å…¼å®¹æ€§ã€‚
"""

import asyncio
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.loom.interpretation.llm_provider import (
    LLMProvider,
    ProviderManager,
    LLMProviderFactory,
)
from src.loom.interpretation.enhanced_provider_manager import (
    EnhancedProviderManager,
    ProviderPriority,
)
from src.loom.interpretation.cost_optimizer import CostOptimizer, BudgetLimit
from src.loom.interpretation.local_model_provider import LocalModelProvider


class MockProvider(LLMProvider):
    """æ¨¡æ‹ŸProviderç”¨äºæµ‹è¯•"""

    def __init__(self, name="mock", success_rate=1.0):
        config = {"name": name, "type": "mock", "model": "mock-model", "enabled": True}
        super().__init__(config)
        self.success_rate = success_rate
        self.call_count = 0

    async def _generate_impl(self, prompt: str, **kwargs) -> LLMResponse:
        """æ¨¡æ‹Ÿç”Ÿæˆå®ç°"""
        self.call_count += 1

        # æ¨¡æ‹Ÿå¤±è´¥
        if self.call_count == 1 and self.success_rate < 1.0:
            raise Exception("Simulated failure for testing")

        from src.loom.interpretation.llm_provider import LLMResponse

        return LLMResponse(
            content=f"Mock response from {self.name} for: {prompt[:50]}...",
            model=self.model,
            usage={"prompt_tokens": 10, "completion_tokens": 20, "total_tokens": 30},
            metadata={"provider": self.name, "mock": True},
        )

    async def generate_stream(self, prompt: str, **kwargs):
        """æ¨¡æ‹Ÿæµå¼ç”Ÿæˆ"""
        yield f"Stream from {self.name}: "
        yield prompt[:20]

    async def health_check(self):
        """å¥åº·æ£€æŸ¥"""
        return {"healthy": True, "latency": 0.1}


async def test_backward_compatibility():
    """æµ‹è¯•å‘åå…¼å®¹æ€§"""
    print("=" * 60)
    print("æµ‹è¯•å‘åå…¼å®¹æ€§")
    print("=" * 60)

    # 1. æµ‹è¯•ç°æœ‰çš„ProviderManagerä»ç„¶å·¥ä½œ
    print("\n1. æµ‹è¯•ç°æœ‰çš„ProviderManager...")
    manager = ProviderManager()

    mock_provider = MockProvider("legacy_provider")
    manager.register_provider("legacy", mock_provider)
    manager.set_default("legacy")

    assert "legacy" in manager.providers
    print("  âœ“ ç°æœ‰ProviderManageræ³¨å†ŒProvideræˆåŠŸ")

    # 2. æµ‹è¯•EnhancedProviderManagerç»§æ‰¿è‡ªProviderManager
    print("\n2. æµ‹è¯•EnhancedProviderManagerç»§æ‰¿å…³ç³»...")
    enhanced_config = {
        "health_check_interval": 5,
        "selection_strategy": "weighted_round_robin",
    }
    enhanced_manager = EnhancedProviderManager(enhanced_config)

    # éªŒè¯EnhancedProviderManageræ˜¯ProviderManagerçš„å­ç±»
    assert isinstance(enhanced_manager, ProviderManager)
    print("  âœ“ EnhancedProviderManageræ˜¯ProviderManagerçš„å­ç±»")

    # 3. æµ‹è¯•EnhancedProviderManageræ”¯æŒç°æœ‰æ¥å£
    print("\n3. æµ‹è¯•EnhancedProviderManageræ”¯æŒç°æœ‰æ¥å£...")
    await enhanced_manager.register_provider("enhanced_test", mock_provider)
    enhanced_manager.set_default("enhanced_test")

    # æµ‹è¯•ç°æœ‰çš„generate_with_fallbackæ–¹æ³•
    try:
        response = await enhanced_manager.generate_with_fallback("Test prompt")
        print(f"  âœ“ ç°æœ‰generate_with_fallbackæ–¹æ³•å·¥ä½œæ­£å¸¸")
        print(f"    å“åº”: {response.content[:50]}...")
    except Exception as e:
        print(f"  âœ— generate_with_fallbackå¤±è´¥: {e}")
        return False

    # 4. æµ‹è¯•æ–°çš„æ™ºèƒ½æ•…éšœè½¬ç§»æ–¹æ³•
    print("\n4. æµ‹è¯•æ–°çš„æ™ºèƒ½æ•…éšœè½¬ç§»æ–¹æ³•...")
    try:
        response = await enhanced_manager.generate_with_intelligent_fallback(
            "Test intelligent fallback", priority=ProviderPriority.BALANCED
        )
        print(f"  âœ“ æ™ºèƒ½æ•…éšœè½¬ç§»æ–¹æ³•å·¥ä½œæ­£å¸¸")
        print(f"    å“åº”: {response.content[:50]}...")
    except Exception as e:
        print(f"  âœ— æ™ºèƒ½æ•…éšœè½¬ç§»å¤±è´¥: {e}")
        return False

    return True


async def test_cost_optimizer_integration():
    """æµ‹è¯•CostOptimizeré›†æˆ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•CostOptimizeré›†æˆ")
    print("=" * 60)

    # 1. åˆ›å»ºCostOptimizer
    print("\n1. åˆ›å»ºCostOptimizer...")
    cost_config = {
        "budget": {
            "total_budget": 100.0,
            "daily_limit": 10.0,
            "monthly_limit": 50.0,
            "per_request_limit": 1.0,
        },
        "pricing": {
            "mock": {"model_pricing": {"mock-model": {"input": 0.001, "output": 0.002}}}
        },
    }

    cost_optimizer = CostOptimizer(cost_config)
    print("  âœ“ CostOptimizeråˆ›å»ºæˆåŠŸ")

    # 2. æµ‹è¯•æˆæœ¬è®°å½•
    print("\n2. æµ‹è¯•æˆæœ¬è®°å½•...")
    from src.loom.interpretation.llm_provider import LLMResponse

    mock_response = LLMResponse(
        content="Test response",
        model="mock-model",
        usage={"prompt_tokens": 100, "completion_tokens": 200, "total_tokens": 300},
        metadata={"provider": "mock"},
    )

    cost = cost_optimizer.record_usage("mock", mock_response)
    print(f"  âœ“ æˆæœ¬è®°å½•æˆåŠŸ: ${cost:.6f}")
    print(f"    æ€»è®°å½•æ•°: {len(cost_optimizer.cost_history)}")

    # 3. æµ‹è¯•é¢„ç®—æ£€æŸ¥
    print("\n3. æµ‹è¯•é¢„ç®—æ£€æŸ¥...")
    can_make, reason = cost_optimizer.can_make_request(estimated_cost=0.5)
    print(f"  âœ“ é¢„ç®—æ£€æŸ¥: {can_make} - {reason}")

    # 4. æµ‹è¯•æˆæœ¬æ‘˜è¦
    print("\n4. æµ‹è¯•æˆæœ¬æ‘˜è¦...")
    summary = cost_optimizer.get_cost_summary()
    print(f"  âœ“ æˆæœ¬æ‘˜è¦ç”ŸæˆæˆåŠŸ")
    print(f"    æ€»æˆæœ¬: ${summary['total_cost']:.4f}")
    print(f"    è¯·æ±‚æ•°: {summary['request_count']}")

    return True


async def test_local_model_provider_compatibility():
    """æµ‹è¯•LocalModelProviderå…¼å®¹æ€§"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•LocalModelProviderå…¼å®¹æ€§")
    print("=" * 60)

    # 1. æµ‹è¯•LocalModelProviderç»§æ‰¿å…³ç³»
    print("\n1. æµ‹è¯•LocalModelProviderç»§æ‰¿å…³ç³»...")
    from src.loom.interpretation.llm_provider import LocalProvider

    local_config = {
        "name": "test_local",
        "type": "local",
        "model": "test-model",
        "base_url": "http://localhost:11434/api",
        "auto_discovery": False,  # æµ‹è¯•ä¸­ç¦ç”¨è‡ªåŠ¨å‘ç°
    }

    local_provider = LocalModelProvider(local_config)

    # éªŒè¯LocalModelProvideræ˜¯LocalProviderçš„å­ç±»
    assert isinstance(local_provider, LocalProvider)
    print("  âœ“ LocalModelProvideræ˜¯LocalProviderçš„å­ç±»")

    # 2. æµ‹è¯•æ¨¡å‹ç®¡ç†å™¨åŠŸèƒ½
    print("\n2. æµ‹è¯•æ¨¡å‹ç®¡ç†å™¨åŠŸèƒ½...")
    assert local_provider.model_manager is not None
    print("  âœ“ æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")

    # 3. æµ‹è¯•æ€§èƒ½ç›‘æ§
    print("\n3. æµ‹è¯•æ€§èƒ½ç›‘æ§...")
    assert local_provider.performance_monitoring is True
    print("  âœ“ æ€§èƒ½ç›‘æ§å·²å¯ç”¨")

    # 4. æµ‹è¯•å¯ç”¨æ¨¡å‹è·å–
    print("\n4. æµ‹è¯•å¯ç”¨æ¨¡å‹è·å–...")
    models = await local_provider.get_available_models()
    print(f"  âœ“ è·å–åˆ° {len(models)} ä¸ªæ¨¡å‹")

    await local_provider.close()
    print("  âœ“ LocalModelProviderå…³é—­æˆåŠŸ")

    return True


async def test_integration_scenario():
    """æµ‹è¯•é›†æˆåœºæ™¯"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæ•´é›†æˆåœºæ™¯")
    print("=" * 60)

    # åˆ›å»ºå®Œæ•´çš„å·¥ä½œæµ
    print("\n1. åˆ›å»ºé›†æˆå·¥ä½œæµ...")

    # åˆ›å»ºEnhancedProviderManager
    manager_config = {
        "health_check_interval": 2,
        "selection_strategy": "weighted_round_robin",
        "fallback_order": ["provider2", "provider3"],
        "fallback_delay": 0.1,
    }

    manager = EnhancedProviderManager(manager_config)

    # åˆ›å»ºCostOptimizer
    cost_config = {
        "budget": {"total_budget": 50.0, "daily_limit": 5.0},
        "optimization_enabled": True,
    }
    cost_optimizer = CostOptimizer(cost_config)
    manager.cost_tracker = cost_optimizer

    # æ³¨å†Œå¤šä¸ªProvider
    providers = [
        MockProvider("provider1", success_rate=0.8),
        MockProvider("provider2", success_rate=1.0),
        MockProvider("provider3", success_rate=0.9),
    ]

    for i, provider in enumerate(providers, 1):
        await manager.register_provider(f"provider{i}", provider)

    manager.set_default("provider1")
    manager.set_fallback_order(["provider2", "provider3"])

    print("  âœ“ å·¥ä½œæµç»„ä»¶åˆ›å»ºå®Œæˆ")
    print(f"    æ³¨å†Œäº† {len(providers)} ä¸ªProvider")
    print(f"    æˆæœ¬ä¼˜åŒ–å™¨: {'å·²é›†æˆ' if manager.cost_tracker else 'æœªé›†æˆ'}")

    # æµ‹è¯•å¤šä¸ªè¯·æ±‚
    print("\n2. æµ‹è¯•å¤šä¸ªè¯·æ±‚...")
    test_prompts = [
        "First test prompt",
        "Second test prompt with longer text",
        "Third prompt for fallback testing",
    ]

    successful_requests = 0
    for i, prompt in enumerate(test_prompts, 1):
        try:
            response = await manager.generate_with_intelligent_fallback(
                prompt, priority=ProviderPriority.BALANCED
            )
            print(f"  âœ“ è¯·æ±‚ {i} æˆåŠŸ: {response.content[:40]}...")
            successful_requests += 1
        except Exception as e:
            print(f"  âœ— è¯·æ±‚ {i} å¤±è´¥: {e}")

    print(f"\n  æˆåŠŸç‡: {successful_requests}/{len(test_prompts)}")

    # æ£€æŸ¥æˆæœ¬è®°å½•
    print("\n3. æ£€æŸ¥æˆæœ¬è®°å½•...")
    cost_summary = cost_optimizer.get_cost_summary()
    print(f"  æ€»æˆæœ¬: ${cost_summary['total_cost']:.4f}")
    print(f"  æ€»è¯·æ±‚æ•°: {cost_summary['request_count']}")

    # æ£€æŸ¥Providerç»Ÿè®¡
    print("\n4. æ£€æŸ¥Providerç»Ÿè®¡...")
    stats = await manager.get_provider_stats()
    print(f"  å¥åº·Provideræ•°: {stats['overall']['healthy_providers']}/{len(providers)}")
    print(f"  æ€»ä½“æˆåŠŸç‡: {stats['overall']['success_rate']:.1%}")

    # æ¸…ç†
    await manager.close_all()
    print("\n5. æ¸…ç†èµ„æº...")
    print("  âœ“ æ‰€æœ‰èµ„æºå·²æ¸…ç†")

    return successful_requests > 0


async def main():
    """ä¸»å‡½æ•°"""
    print("LLM Providerå¢å¼ºç»„ä»¶é›†æˆéªŒè¯")
    print("=" * 60)

    tests = [
        ("å‘åå…¼å®¹æ€§æµ‹è¯•", test_backward_compatibility),
        ("CostOptimizeré›†æˆæµ‹è¯•", test_cost_optimizer_integration),
        ("LocalModelProviderå…¼å®¹æ€§æµ‹è¯•", test_local_model_provider_compatibility),
        ("å®Œæ•´é›†æˆåœºæ™¯æµ‹è¯•", test_integration_scenario),
    ]

    results = []
    for test_name, test_func in tests:
        try:
            print(f"\nå¼€å§‹ {test_name}...")
            success = await test_func()
            results.append((test_name, success))

            if success:
                print(f"âœ“ {test_name} é€šè¿‡")
            else:
                print(f"âœ— {test_name} å¤±è´¥")

        except Exception as e:
            print(f"âœ— {test_name} å¼‚å¸¸: {e}")
            import traceback

            traceback.print_exc()
            results.append((test_name, False))

    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)

    passed = sum(1 for _, success in results if success)
    total = len(results)

    for test_name, success in results:
        status = "âœ“ é€šè¿‡" if success else "âœ— å¤±è´¥"
        print(f"{test_name}: {status}")

    print(f"\næ€»è®¡: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")

    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰é›†æˆéªŒè¯æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print(f"\nâš ï¸  {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")
        return 1


if __name__ == "__main__":
    # è®¾ç½®äº‹ä»¶å¾ªç¯ç­–ç•¥ï¼ˆWindowséœ€è¦ï¼‰
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    # è¿è¡Œæµ‹è¯•
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
