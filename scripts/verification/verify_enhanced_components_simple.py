#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆLLM Providerå¢å¼ºç»„ä»¶éªŒè¯è„šæœ¬

ç›´æ¥éªŒè¯ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶çš„åŠŸèƒ½å’Œå…¼å®¹æ€§ï¼Œä¸ä¾èµ–é¡¹ç›®å…¶ä»–éƒ¨åˆ†ã€‚
"""

import asyncio
import os
import sys
from pathlib import Path

# ç›´æ¥å¯¼å…¥æˆ‘ä»¬å®ç°çš„ç»„ä»¶
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from dataclasses import dataclass, field
from datetime import datetime

# å®šä¹‰å¿…è¦çš„åŸºç±»å’Œæ•°æ®ç»“æ„
from typing import Any, AsyncGenerator, Dict, List, Optional

import aiohttp


# å®šä¹‰LLMResponseï¼ˆç®€åŒ–ç‰ˆï¼‰
@dataclass
class LLMResponse:
    content: str
    model: str
    usage: Dict[str, int] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)


# å®šä¹‰LLMProvideråŸºç±»ï¼ˆç®€åŒ–ç‰ˆï¼‰
class LLMProvider:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.name = config.get("name", "unknown")
        self.provider_type = config.get("type", "unknown")
        self.model = config.get("model", "default")
        self.enabled = config.get("enabled", True)
        self.request_count = 0
        self.total_tokens = 0
        self.total_cost = 0.0

    async def generate(self, prompt: str, **kwargs) -> LLMResponse:
        self.request_count += 1
        return await self._generate_impl(prompt, **kwargs)

    async def _generate_impl(self, prompt: str, **kwargs) -> LLMResponse:
        raise NotImplementedError

    async def generate_stream(self, prompt: str, **kwargs):
        raise NotImplementedError

    def get_stats(self):
        return {
            "request_count": self.request_count,
            "total_tokens": self.total_tokens,
            "total_cost": self.total_cost,
        }


# å®šä¹‰LocalProvideråŸºç±»ï¼ˆç®€åŒ–ç‰ˆï¼‰
class LocalProvider(LLMProvider):
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.base_url = config.get("base_url", "http://localhost:11434/api")


# ç°åœ¨å¯¼å…¥æˆ‘ä»¬å®ç°çš„ç»„ä»¶
try:
    from loom.interpretation.cost_optimizer import (
        BudgetAlertLevel,
        BudgetLimit,
        CostOptimizer,
    )
    from loom.interpretation.enhanced_provider_manager import (
        EnhancedProviderManager,
        FallbackStrategy,
        ProviderHealthMonitor,
        ProviderLoadBalancer,
        ProviderPriority,
    )
    from loom.interpretation.local_model_provider import (
        LocalModelInfo,
        LocalModelProvider,
        LocalModelType,
    )

    print("âœ“ æˆåŠŸå¯¼å…¥æ‰€æœ‰å¢å¼ºç»„ä»¶")
except ImportError as e:
    print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)


class MockEnhancedProvider(LLMProvider):
    """æ¨¡æ‹ŸProviderç”¨äºæµ‹è¯•å¢å¼ºåŠŸèƒ½"""

    def __init__(self, name="mock", success=True, latency=0.1):
        config = {"name": name, "type": "mock", "model": "test-model", "enabled": True}
        super().__init__(config)
        self.success = success
        self.latency = latency

    async def _generate_impl(self, prompt: str, **kwargs) -> LLMResponse:
        import asyncio

        await asyncio.sleep(self.latency)

        if not self.success:
            raise Exception(f"Mock failure from {self.name}")

        return LLMResponse(
            content=f"Response from {self.name}: {prompt[:20]}...",
            model=self.model,
            usage={"prompt_tokens": 50, "completion_tokens": 100, "total_tokens": 150},
            metadata={"provider": self.name},
        )

    async def generate_stream(self, prompt: str, **kwargs):
        if not self.success:
            raise Exception(f"Mock stream failure from {self.name}")

        yield f"Stream from {self.name}: "
        for word in prompt.split()[:3]:
            yield word + " "

    async def health_check(self):
        return {"healthy": self.success, "latency": self.latency}


async def test_enhanced_provider_manager():
    """æµ‹è¯•EnhancedProviderManager"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• EnhancedProviderManager")
    print("=" * 60)

    # åˆ›å»ºç®¡ç†å™¨
    config = {
        "health_check_interval": 1,
        "selection_strategy": "weighted_round_robin",
        "fallback_order": ["provider2", "provider3"],
        "fallback_delay": 0.01,
    }

    manager = EnhancedProviderManager(config)
    print("âœ“ EnhancedProviderManager åˆ›å»ºæˆåŠŸ")

    # æ³¨å†Œå¤šä¸ªProvider
    providers = {
        "provider1": MockEnhancedProvider("provider1", success=True, latency=0.1),
        "provider2": MockEnhancedProvider("provider2", success=True, latency=0.2),
        "provider3": MockEnhancedProvider(
            "provider3", success=False, latency=0.1
        ),  # ä¼šå¤±è´¥çš„
    }

    for name, provider in providers.items():
        await manager.register_provider(name, provider)

    print(f"âœ“ æ³¨å†Œäº† {len(providers)} ä¸ªProvider")

    # æµ‹è¯•æ™ºèƒ½æ•…éšœè½¬ç§»
    print("\næµ‹è¯•æ™ºèƒ½æ•…éšœè½¬ç§»...")
    try:
        response = await manager.generate_with_intelligent_fallback(
            "Test prompt for intelligent fallback", priority=ProviderPriority.BALANCED
        )
        print(f"âœ“ æ™ºèƒ½æ•…éšœè½¬ç§»æˆåŠŸ: {response.content[:50]}...")

        # éªŒè¯å“åº”æ¥è‡ªå¯ç”¨çš„Providerï¼ˆä¸æ˜¯provider3ï¼‰
        assert "provider3" not in response.content
        print("âœ“ æ•…éšœè½¬ç§»é€»è¾‘æ­£ç¡®ï¼ˆé¿å¼€äº†å¤±è´¥çš„Providerï¼‰")

    except Exception as e:
        print(f"âœ— æ™ºèƒ½æ•…éšœè½¬ç§»å¤±è´¥: {e}")
        return False

    # æµ‹è¯•Providerç»Ÿè®¡
    print("\næµ‹è¯•Providerç»Ÿè®¡...")
    try:
        stats = await manager.get_provider_stats()
        assert "providers" in stats
        assert "overall" in stats
        print(f"âœ“ è·å–ç»Ÿè®¡æˆåŠŸ: {stats['overall']['total_requests']} ä¸ªè¯·æ±‚")
    except Exception as e:
        print(f"âœ— è·å–ç»Ÿè®¡å¤±è´¥: {e}")
        return False

    # æ¸…ç†
    await manager.close_all()
    print("âœ“ èµ„æºæ¸…ç†æˆåŠŸ")

    return True


async def test_cost_optimizer():
    """æµ‹è¯•CostOptimizer"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• CostOptimizer")
    print("=" * 60)

    # åˆ›å»ºæˆæœ¬ä¼˜åŒ–å™¨
    config = {
        "budget": {
            "total_budget": 100.0,
            "daily_limit": 10.0,
            "monthly_limit": 50.0,
            "per_request_limit": 0.5,
        },
        "pricing": {
            "test_provider": {
                "model_pricing": {"test-model": {"input": 0.001, "output": 0.002}}
            }
        },
        "optimization_enabled": True,
    }

    optimizer = CostOptimizer(config)
    print("âœ“ CostOptimizer åˆ›å»ºæˆåŠŸ")

    # æµ‹è¯•æˆæœ¬è®°å½•
    print("\næµ‹è¯•æˆæœ¬è®°å½•...")
    response = LLMResponse(
        content="Test response",
        model="test-model",
        usage={"prompt_tokens": 100, "completion_tokens": 200, "total_tokens": 300},
        metadata={"provider": "test_provider"},
    )

    cost = optimizer.record_usage("test_provider", response)
    print(f"âœ“ æˆæœ¬è®°å½•æˆåŠŸ: ${cost:.6f}")
    print(f"  å†å²è®°å½•æ•°: {len(optimizer.cost_history)}")

    # æµ‹è¯•é¢„ç®—æ£€æŸ¥
    print("\næµ‹è¯•é¢„ç®—æ£€æŸ¥...")
    can_make, reason = optimizer.can_make_request(estimated_cost=0.1)
    print(f"âœ“ é¢„ç®—æ£€æŸ¥: {can_make} - {reason}")

    # æµ‹è¯•æˆæœ¬æ‘˜è¦
    print("\næµ‹è¯•æˆæœ¬æ‘˜è¦...")
    summary = optimizer.get_cost_summary()
    print(f"âœ“ æˆæœ¬æ‘˜è¦: ${summary['total_cost']:.4f} / {summary['request_count']} è¯·æ±‚")

    # æµ‹è¯•ä¼˜åŒ–å»ºè®®
    print("\næµ‹è¯•ä¼˜åŒ–å»ºè®®...")
    suggestions = optimizer.get_optimization_suggestions()
    print(f"âœ“ ç”Ÿæˆ {len(suggestions)} æ¡ä¼˜åŒ–å»ºè®®")

    return True


async def test_local_model_provider():
    """æµ‹è¯•LocalModelProvider"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• LocalModelProvider")
    print("=" * 60)

    # åˆ›å»ºæœ¬åœ°æ¨¡å‹Provider
    config = {
        "name": "test_local",
        "type": "local",
        "model": "test-model",
        "base_url": "http://localhost:11434/api",
        "auto_discovery": False,  # æµ‹è¯•ä¸­ç¦ç”¨
        "performance_monitoring": True,
        "auto_model_selection": True,
    }

    try:
        provider = LocalModelProvider(config)
        print("âœ“ LocalModelProvider åˆ›å»ºæˆåŠŸ")

        # éªŒè¯ç»§æ‰¿å…³ç³»
        assert isinstance(provider, LocalProvider)
        print("âœ“ æ­£ç¡®çš„ç»§æ‰¿å…³ç³»ï¼ˆLocalModelProvider -> LocalProvider -> LLMProviderï¼‰")

        # éªŒè¯æ¨¡å‹ç®¡ç†å™¨
        assert provider.model_manager is not None
        print("âœ“ æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")

        # æµ‹è¯•æ¨¡å‹å‘ç°ï¼ˆæ¨¡æ‹Ÿï¼‰
        print("\næµ‹è¯•æ¨¡å‹å‘ç°...")
        # æ·»åŠ æ¨¡æ‹Ÿæ¨¡å‹
        test_model = LocalModelInfo(
            name="test-model-7b",
            model_type=LocalModelType.OLLAMA,
            size="7B",
            format="gguf",
            context_length=4096,
            parameters=7_000_000_000,
        )

        provider.model_manager.models["test-model-7b"] = test_model
        models = await provider.get_available_models()
        print(f"âœ“ å‘ç° {len(models)} ä¸ªæ¨¡å‹")

        # æµ‹è¯•æ¨¡å‹æ¨è
        print("\næµ‹è¯•æ¨¡å‹æ¨è...")
        recommended = await provider.model_manager.get_recommended_model()
        print(f"âœ“ æ¨èæ¨¡å‹: {recommended}")

        # æ¸…ç†
        await provider.close()
        print("âœ“ èµ„æºæ¸…ç†æˆåŠŸ")

        return True

    except Exception as e:
        print(f"âœ— LocalModelProvideræµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


async def test_integration():
    """æµ‹è¯•é›†æˆ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»„ä»¶é›†æˆ")
    print("=" * 60)

    # åˆ›å»ºé›†æˆçš„åœºæ™¯
    print("åˆ›å»ºé›†æˆå·¥ä½œæµ...")

    # 1. åˆ›å»ºEnhancedProviderManager
    manager_config = {
        "health_check_interval": 1,
        "selection_strategy": "weighted_round_robin",
    }
    manager = EnhancedProviderManager(manager_config)

    # 2. åˆ›å»ºCostOptimizerå¹¶é›†æˆ
    cost_config = {"budget": {"total_budget": 50.0, "daily_limit": 5.0}}
    cost_optimizer = CostOptimizer(cost_config)
    manager.cost_tracker = cost_optimizer

    # 3. æ³¨å†ŒProvider
    provider1 = MockEnhancedProvider("integrated1", success=True)
    provider2 = MockEnhancedProvider("integrated2", success=True)

    await manager.register_provider("provider1", provider1)
    await manager.register_provider("provider2", provider2)

    manager.set_default("provider1")
    manager.set_fallback_order(["provider2"])

    print("âœ“ é›†æˆå·¥ä½œæµåˆ›å»ºå®Œæˆ")
    print(f"  Provideræ•°: {len(manager.providers)}")
    print(f"  æˆæœ¬ä¼˜åŒ–å™¨: {'å·²é›†æˆ' if manager.cost_tracker else 'æœªé›†æˆ'}")

    # æµ‹è¯•é›†æˆåŠŸèƒ½
    print("\næµ‹è¯•é›†æˆåŠŸèƒ½...")

    # ç”Ÿæˆè¯·æ±‚
    response = await manager.generate_with_intelligent_fallback(
        "Integration test prompt", priority=ProviderPriority.COST
    )

    print(f"âœ“ é›†æˆç”ŸæˆæˆåŠŸ: {response.content[:40]}...")

    # éªŒè¯æˆæœ¬è®°å½•
    assert len(cost_optimizer.cost_history) > 0
    print(f"âœ“ æˆæœ¬è®°å½•æˆåŠŸ: {len(cost_optimizer.cost_history)} æ¡è®°å½•")

    # è·å–ç»Ÿè®¡
    stats = await manager.get_provider_stats()
    print(f"âœ“ ç»Ÿè®¡è·å–æˆåŠŸ: {stats['overall']['total_requests']} æ€»è¯·æ±‚")

    # æ¸…ç†
    await manager.close_all()
    print("âœ“ é›†æˆèµ„æºæ¸…ç†æˆåŠŸ")

    return True


async def main():
    """ä¸»å‡½æ•°"""
    print("LLM Providerå¢å¼ºç»„ä»¶éªŒè¯")
    print("=" * 60)

    tests = [
        ("EnhancedProviderManager", test_enhanced_provider_manager),
        ("CostOptimizer", test_cost_optimizer),
        ("LocalModelProvider", test_local_model_provider),
        ("ç»„ä»¶é›†æˆ", test_integration),
    ]

    results = []
    for name, test_func in tests:
        try:
            print(f"\nå¼€å§‹æµ‹è¯•: {name}")
            success = await test_func()
            results.append((name, success))

            if success:
                print(f"âœ“ {name} æµ‹è¯•é€šè¿‡")
            else:
                print(f"âœ— {name} æµ‹è¯•å¤±è´¥")

        except Exception as e:
            print(f"âœ— {name} æµ‹è¯•å¼‚å¸¸: {e}")
            import traceback

            traceback.print_exc()
            results.append((name, False))

    # æ€»ç»“
    print("\n" + "=" * 60)
    print("éªŒè¯æ€»ç»“")
    print("=" * 60)

    passed = sum(1 for _, success in results if success)
    total = len(results)

    for name, success in results:
        status = "âœ“ é€šè¿‡" if success else "âœ— å¤±è´¥"
        print(f"{name:30} {status}")

    print(f"\næ€»è®¡: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")

    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰ç»„ä»¶éªŒè¯é€šè¿‡ï¼")
        print("\nå®ç°æ€»ç»“:")
        print("1. EnhancedProviderManager - æ™ºèƒ½æ•…éšœè½¬ç§»ã€å¥åº·ç›‘æ§ã€è´Ÿè½½å‡è¡¡")
        print("2. CostOptimizer - æˆæœ¬è·Ÿè¸ªã€é¢„ç®—æ§åˆ¶ã€ä¼˜åŒ–å»ºè®®")
        print("3. LocalModelProvider - æœ¬åœ°æ¨¡å‹æ”¯æŒã€è‡ªåŠ¨å‘ç°ã€æ€§èƒ½ç›‘æ§")
        print("\næ‰€æœ‰ç»„ä»¶å·²æˆåŠŸé›†æˆå¹¶ä¿æŒå‘åå…¼å®¹æ€§ã€‚")
        return 0
    else:
        print(f"\nâš ï¸  {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")
        return 1


if __name__ == "__main__":
    # Windowsäº‹ä»¶å¾ªç¯ç­–ç•¥
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    exit_code = asyncio.run(main())
    sys.exit(exit_code)
