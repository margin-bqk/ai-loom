#!/usr/bin/env python3
"""
å¢å¼ºæ¨ç†å¼•æ“é›†æˆæµ‹è¯•

éªŒè¯æ–°ç»„ä»¶ä¸ç°æœ‰ç³»ç»Ÿçš„å…¼å®¹æ€§ã€‚
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.loom.interpretation import (
    # åŸºç¡€ç»„ä»¶
    ReasoningPipeline,
    ReasoningContext,
    ReasoningResult,
    ConsistencyChecker,
    # å¢å¼ºç»„ä»¶
    EnhancedReasoningPipeline,
    EnhancedContextBuilder,
    EnhancedConsistencyChecker,
    ReasoningTracker,
    # æšä¸¾å’Œæ•°æ®ç±»å‹
    ContextOptimizationStrategy,
    ReasoningStepType,
    DecisionImportance,
)

from src.loom.interpretation.llm_provider import LLMProvider, LLMResponse


class MockLLMProvider(LLMProvider):
    """æ¨¡æ‹ŸLLMæä¾›è€…ç”¨äºæµ‹è¯•"""

    def __init__(self, name="mock_provider"):
        self.name = name
        self.call_count = 0
        self.provider_type = "mock"

    async def _generate_impl(self, prompt: str, **kwargs) -> LLMResponse:
        """ç”Ÿæˆæ–‡æœ¬çš„å…·ä½“å®ç°"""
        self.call_count += 1

        # åŸºäºæç¤ºç”Ÿæˆç®€å•å“åº”
        if "åŸå ¡" in prompt:
            response = "ç©å®¶æ¢ç´¢äº†å¤è€çš„åŸå ¡ï¼Œå‘ç°äº†éšè—çš„é€šé“ã€‚å®ˆå«è­¦æƒ•åœ°å·¡é€»ï¼Œä½†ç©å®¶æˆåŠŸé¿å¼€äº†ä»–ä»¬ã€‚"
        elif "æ£®æ—" in prompt:
            response = "ç©å®¶è¿›å…¥äº†ç¥ç§˜çš„æ£®æ—ï¼Œæ ‘æœ¨é«˜è€¸å…¥äº‘ã€‚è¿œå¤„ä¼ æ¥å¥‡æ€ªçš„å£°å“ï¼Œä½†ç©å®¶å†³å®šç»§ç»­å‰è¿›ã€‚"
        else:
            response = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å™äº‹å“åº”ã€‚ç©å®¶è¿›è¡Œäº†è¡ŒåŠ¨ï¼Œæ•…äº‹ç»§ç»­å‘å±•ã€‚"

        return LLMResponse(
            content=response,
            model=kwargs.get("model", "mock-model"),
            usage={
                "prompt_tokens": len(prompt) // 4,
                "completion_tokens": len(response) // 4,
            },
            metadata={"mock": True, "call_count": self.call_count},
        )

    async def generate_stream(self, prompt: str, **kwargs):
        """æµå¼ç”Ÿæˆæ–‡æœ¬"""
        import asyncio

        response = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æµå¼å“åº”ã€‚"
        for char in response:
            await asyncio.sleep(0.01)
            yield char

    def get_available_models(self):
        return ["mock-model-1", "mock-model-2"]

    def get_token_count(self, text):
        return len(text) // 4

    async def validate_connection(self):
        return True


async def test_backward_compatibility():
    """æµ‹è¯•å‘åå…¼å®¹æ€§"""
    print("=" * 60)
    print("æµ‹è¯•å‘åå…¼å®¹æ€§")
    print("=" * 60)

    # åˆ›å»ºæ¨¡æ‹ŸLLMæä¾›è€…
    mock_llm = MockLLMProvider()

    # æµ‹è¯•åŸºç¡€æ¨ç†ç®¡é“ä»ç„¶å·¥ä½œ
    print("\n1. æµ‹è¯•åŸºç¡€ReasoningPipeline...")
    try:
        base_pipeline = ReasoningPipeline(llm_provider=mock_llm)

        context = ReasoningContext(
            session_id="compat_test",
            turn_number=1,
            player_input="ç©å®¶æƒ³è¦æ¢ç´¢åŸå ¡ã€‚",
            rules_text="è¿™æ˜¯ä¸€ä¸ªå¥‡å¹»ä¸–ç•Œã€‚",
            memories=[],
            interventions=[],
        )

        result = await base_pipeline.process(context)
        print(f"   åŸºç¡€ç®¡é“æµ‹è¯•é€šè¿‡")
        print(f"   å“åº”é•¿åº¦: {len(result.narrative_response)}")
        print(f"   ç½®ä¿¡åº¦: {result.confidence:.2f}")
    except Exception as e:
        print(f"   åŸºç¡€ç®¡é“æµ‹è¯•å¤±è´¥: {e}")
        return False

    # æµ‹è¯•åŸºç¡€ä¸€è‡´æ€§æ£€æŸ¥å™¨
    print("\n2. æµ‹è¯•åŸºç¡€ConsistencyChecker...")
    try:
        base_checker = ConsistencyChecker()

        report = base_checker.check(
            response="æµ‹è¯•å“åº”", rules_text="æµ‹è¯•è§„åˆ™", constraints=[]
        )

        print(f"   åŸºç¡€æ£€æŸ¥å™¨æµ‹è¯•é€šè¿‡")
        print(f"   ä¸€è‡´æ€§åˆ†æ•°: {report.get('score', 0):.2f}")
    except Exception as e:
        print(f"   åŸºç¡€æ£€æŸ¥å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

    return True


async def test_enhanced_components():
    """æµ‹è¯•å¢å¼ºç»„ä»¶"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•å¢å¼ºç»„ä»¶")
    print("=" * 60)

    mock_llm = MockLLMProvider()

    # æµ‹è¯•å¢å¼ºæ¨ç†ç®¡é“
    print("\n1. æµ‹è¯•EnhancedReasoningPipeline...")
    try:
        enhanced_pipeline = EnhancedReasoningPipeline(llm_provider=mock_llm)

        context = ReasoningContext(
            session_id="enhanced_test",
            turn_number=1,
            player_input="ç©å®¶æƒ³è¦ä¸å®ˆå«äº¤è°ˆã€‚",
            rules_text="è¿™æ˜¯ä¸€ä¸ªä¸­ä¸–çºªå¥‡å¹»ä¸–ç•Œã€‚é­”æ³•å­˜åœ¨ä½†ç¨€æœ‰ã€‚",
            memories=[
                {
                    "type": "character",
                    "content": {"name": "å®ˆå«", "traits": ["å¿ è¯š", "è­¦æƒ•"]},
                }
            ],
            interventions=[],
        )

        result = await enhanced_pipeline.process(context)
        print(f"   å¢å¼ºç®¡é“æµ‹è¯•é€šè¿‡")
        print(f"   å“åº”: {result.narrative_response[:50]}...")
        print(f"   å¢å¼ºç½®ä¿¡åº¦: {result.confidence:.2f}")
        print(f"   è¯¦ç»†æ­¥éª¤: {len(result.reasoning_steps_detailed)}ä¸ª")

        if hasattr(result, "consistency_report"):
            print(
                f"   ä¸€è‡´æ€§æŠ¥å‘Š: {result.consistency_report.get('overall_score', 0):.2f}"
            )
    except Exception as e:
        print(f"   å¢å¼ºç®¡é“æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False

    # æµ‹è¯•å¢å¼ºä¸Šä¸‹æ–‡æ„å»ºå™¨
    print("\n2. æµ‹è¯•EnhancedContextBuilder...")
    try:
        context_builder = EnhancedContextBuilder()

        context = ReasoningContext(
            session_id="context_test",
            turn_number=1,
            player_input="æµ‹è¯•è¾“å…¥",
            rules_text="æµ‹è¯•è§„åˆ™æ–‡æœ¬ã€‚",
            memories=[],
            interventions=[],
        )

        interpretation = type(
            "MockInterpretation",
            (),
            {"constraints": [], "narrative_output": "æµ‹è¯•è§£é‡Š"},
        )()

        # æµ‹è¯•ä¸åŒç­–ç•¥
        strategies = list(ContextOptimizationStrategy)
        for strategy in strategies[:2]:  # æµ‹è¯•å‰ä¸¤ç§ç­–ç•¥
            prompt = await context_builder.build_with_strategy(
                context, interpretation, [], strategy
            )
            print(f"   ç­–ç•¥ '{strategy.value}': {len(prompt)} å­—ç¬¦")

        # åˆ†ææç¤ºè´¨é‡
        test_prompt = "# æµ‹è¯•\nè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æç¤ºã€‚"
        analysis = context_builder.analyze_prompt_quality(test_prompt)
        print(f"   æç¤ºè´¨é‡åˆ†æ: åˆ†æ•°={analysis.get('quality_score', 0):.2f}")

    except Exception as e:
        print(f"   å¢å¼ºä¸Šä¸‹æ–‡æ„å»ºå™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

    # æµ‹è¯•å¢å¼ºä¸€è‡´æ€§æ£€æŸ¥å™¨
    print("\n3. æµ‹è¯•EnhancedConsistencyChecker...")
    try:
        checker = EnhancedConsistencyChecker(llm_provider=mock_llm)

        context = ReasoningContext(
            session_id="consistency_test",
            turn_number=1,
            player_input="æµ‹è¯•",
            rules_text="è§„åˆ™ï¼šç¦æ­¢çŸ›ç›¾ã€‚",
            memories=[],
            interventions=[],
        )

        interpretation = type("MockInterpretation", (), {"constraints": []})()

        report = await checker.deep_check(
            "è¿™æ˜¯ä¸€ä¸ªæ²¡æœ‰çŸ›ç›¾çš„æµ‹è¯•å“åº”ã€‚", context, interpretation, []
        )

        print(f"   å¢å¼ºä¸€è‡´æ€§æ£€æŸ¥å™¨æµ‹è¯•é€šè¿‡")
        print(f"   æ€»ä½“åˆ†æ•°: {report.overall_score:.2f}")
        print(f"   é€šè¿‡: {report.passed}")
        print(f"   é—®é¢˜æ•°é‡: {len(report.issues)}")

    except Exception as e:
        print(f"   å¢å¼ºä¸€è‡´æ€§æ£€æŸ¥å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

    # æµ‹è¯•æ¨ç†è·Ÿè¸ªå™¨
    print("\n4. æµ‹è¯•ReasoningTracker...")
    try:
        tracker = ReasoningTracker(session_id="tracker_test", turn_number=1)

        # åˆ›å»ºå®Œæ•´è½¨è¿¹
        trace_id = tracker.start_trace(metadata={"test": True})

        step_id = tracker.start_step(
            name="é›†æˆæµ‹è¯•æ­¥éª¤",
            step_type=ReasoningStepType.LLM_GENERATION,
            input_data={"test": "data"},
        )

        tracker.end_step(step_id, confidence=0.8)

        tracker.record_decision(
            step_id=step_id,
            description="æµ‹è¯•å†³ç­–",
            alternatives=["A", "B"],
            chosen_alternative="A",
            reasoning="æµ‹è¯•æ¨ç†",
            importance=DecisionImportance.MEDIUM,
            confidence=0.7,
            constraints_applied=["çº¦æŸ1"],
        )

        trace = tracker.end_trace()

        print(f"   æ¨ç†è·Ÿè¸ªå™¨æµ‹è¯•é€šè¿‡")
        print(f"   è½¨è¿¹ID: {trace.trace_id}")
        print(f"   æ­¥éª¤æ•°é‡: {len(trace.steps)}")
        print(f"   å†³ç­–æ•°é‡: {len(trace.decisions)}")
        print(f"   æ€»æ—¶é•¿: {trace.total_duration:.2f}s")

        # ç”ŸæˆæŠ¥å‘Š
        report = tracker.generate_explainability_report(trace_id)
        print(f"   æŠ¥å‘Šç”ŸæˆæˆåŠŸ: {len(report.keys())} ä¸ªéƒ¨åˆ†")

    except Exception as e:
        print(f"   æ¨ç†è·Ÿè¸ªå™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

    return True


async def test_integration_scenarios():
    """æµ‹è¯•é›†æˆåœºæ™¯"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•é›†æˆåœºæ™¯")
    print("=" * 60)

    mock_llm = MockLLMProvider()

    # åœºæ™¯1ï¼šå®Œæ•´æ¨ç†æµç¨‹
    print("\n1. å®Œæ•´æ¨ç†æµç¨‹åœºæ™¯...")
    try:
        # åˆ›å»ºæ‰€æœ‰ç»„ä»¶
        pipeline = EnhancedReasoningPipeline(llm_provider=mock_llm)
        context_builder = EnhancedContextBuilder()
        consistency_checker = EnhancedConsistencyChecker(llm_provider=mock_llm)
        tracker = ReasoningTracker(session_id="integration_scenario", turn_number=1)

        # å¼€å§‹è·Ÿè¸ª
        tracker.start_trace(metadata={"scenario": "full_reasoning"})

        # åˆ›å»ºä¸Šä¸‹æ–‡
        context = ReasoningContext(
            session_id="scenario_1",
            turn_number=1,
            player_input="ç©å®¶å‘ç°äº†ç¥ç§˜çš„åœ°å›¾ï¼Œæƒ³è¦æŒ‰ç…§åœ°å›¾æ¢ç´¢ã€‚",
            rules_text="è¿™æ˜¯ä¸€ä¸ªå†’é™©ä¸–ç•Œã€‚åœ°å›¾å¯èƒ½æŒ‡å‘å®è—æˆ–å±é™©ã€‚",
            memories=[
                {
                    "type": "fact",
                    "content": {"summary": "ç©å®¶ä¹‹å‰æ‰¾åˆ°è¿‡è—å®å›¾", "relevance": 0.8},
                }
            ],
            interventions=[],
        )

        # è®°å½•æ¨ç†å¼€å§‹
        step_id = tracker.start_step(
            name="å®Œæ•´æ¨ç†", step_type=ReasoningStepType.LLM_GENERATION
        )

        # æ‰§è¡Œæ¨ç†
        result = await pipeline.process(context)

        # è®°å½•ç»“æœ
        tracker.end_step(step_id, confidence=result.confidence)

        # æ£€æŸ¥ä¸€è‡´æ€§
        interpretation = type("MockInterpretation", (), {"constraints": []})()

        consistency_report = await consistency_checker.deep_check(
            result.narrative_response, context, interpretation, context.memories
        )

        # è®°å½•å†³ç­–
        tracker.record_decision(
            step_id=step_id,
            description="ç”Ÿæˆæ¢ç´¢å™äº‹",
            alternatives=["å®‰å…¨è·¯çº¿", "å†’é™©è·¯çº¿"],
            chosen_alternative="å†’é™©è·¯çº¿",
            reasoning="ç¬¦åˆå†’é™©ä¸–ç•Œè®¾å®š",
            importance=DecisionImportance.HIGH,
            confidence=result.confidence,
            constraints_applied=["å†’é™©ä¸»é¢˜"],
        )

        # ç»“æŸè·Ÿè¸ª
        trace = tracker.end_trace()

        print(f"   åœºæ™¯1æµ‹è¯•é€šè¿‡")
        print(f"   æ¨ç†ç»“æœç½®ä¿¡åº¦: {result.confidence:.2f}")
        print(f"   ä¸€è‡´æ€§åˆ†æ•°: {consistency_report.overall_score:.2f}")
        print(f"   è·Ÿè¸ªæ­¥éª¤: {len(trace.steps)}")

    except Exception as e:
        print(f"   åœºæ™¯1æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False

    # åœºæ™¯2ï¼šæ‰¹é‡å¤„ç†
    print("\n2. æ‰¹é‡å¤„ç†åœºæ™¯...")
    try:
        pipeline = EnhancedReasoningPipeline(llm_provider=mock_llm)

        contexts = []
        for i in range(2):  # åˆ›å»º2ä¸ªä¸Šä¸‹æ–‡
            contexts.append(
                ReasoningContext(
                    session_id=f"batch_{i}",
                    turn_number=i + 1,
                    player_input=f"æ‰¹é‡æµ‹è¯•è¾“å…¥{i}",
                    rules_text="æ‰¹é‡æµ‹è¯•è§„åˆ™",
                    memories=[],
                    interventions=[],
                )
            )

        results = await pipeline.batch_process(contexts)

        print(f"   åœºæ™¯2æµ‹è¯•é€šè¿‡")
        print(f"   å¤„ç†æ•°é‡: {len(results)}")
        for i, result in enumerate(results):
            print(
                f"     ç»“æœ{i}: ç½®ä¿¡åº¦={result.confidence:.2f}, é•¿åº¦={len(result.narrative_response)}"
            )

    except Exception as e:
        print(f"   åœºæ™¯2æµ‹è¯•å¤±è´¥: {e}")
        return False

    # åœºæ™¯3ï¼šé”™è¯¯å¤„ç†
    print("\n3. é”™è¯¯å¤„ç†åœºæ™¯...")
    try:
        tracker = ReasoningTracker(session_id="error_scenario", turn_number=1)
        tracker.start_trace()

        step_id = tracker.start_step("é”™è¯¯æµ‹è¯•", ReasoningStepType.ERROR_HANDLING)

        # è®°å½•é”™è¯¯
        tracker.record_error(
            step_id=step_id,
            error_type="æ¨¡æ‹Ÿé”™è¯¯",
            error_message="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•é”™è¯¯",
            severity="medium",
            recovery_action="é‡è¯•æ“ä½œ",
            metadata={"test": True},
        )

        tracker.end_step(step_id)
        trace = tracker.end_trace()

        # åˆ†æé”™è¯¯
        error_analysis = tracker.generate_explainability_report(trace.trace_id)

        print(f"   åœºæ™¯3æµ‹è¯•é€šè¿‡")
        print(f"   é”™è¯¯è®°å½•æˆåŠŸ")
        if "error_analysis" in error_analysis:
            print(
                f"   é”™è¯¯åˆ†æ: {error_analysis['error_analysis'].get('total_errors', 0)} ä¸ªé”™è¯¯"
            )

    except Exception as e:
        print(f"   åœºæ™¯3æµ‹è¯•å¤±è´¥: {e}")
        return False

    return True


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("AI-Loom å¢å¼ºæ¨ç†å¼•æ“é›†æˆæµ‹è¯•")
    print("=" * 60)

    all_passed = True

    # è¿è¡Œæµ‹è¯•
    if not await test_backward_compatibility():
        all_passed = False
        print("\nâš ï¸  å‘åå…¼å®¹æ€§æµ‹è¯•å¤±è´¥")
    else:
        print("\nâœ… å‘åå…¼å®¹æ€§æµ‹è¯•é€šè¿‡")

    if not await test_enhanced_components():
        all_passed = False
        print("\nâš ï¸  å¢å¼ºç»„ä»¶æµ‹è¯•å¤±è´¥")
    else:
        print("\nâœ… å¢å¼ºç»„ä»¶æµ‹è¯•é€šè¿‡")

    if not await test_integration_scenarios():
        all_passed = False
        print("\nâš ï¸  é›†æˆåœºæ™¯æµ‹è¯•å¤±è´¥")
    else:
        print("\nâœ… é›†æˆåœºæ™¯æµ‹è¯•é€šè¿‡")

    print("\n" + "=" * 60)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡ï¼å¢å¼ºæ¨ç†å¼•æ“å·²æˆåŠŸé›†æˆã€‚")
        print("\nå®ç°æ€»ç»“:")
        print("1. EnhancedReasoningPipeline: å¤šæ­¥éª¤æ¨ç†ç®¡é“ âœ“")
        print("2. EnhancedContextBuilder: æ™ºèƒ½ä¸Šä¸‹æ–‡æ„å»ºå™¨ âœ“")
        print("3. EnhancedConsistencyChecker: æ·±åº¦ä¸€è‡´æ€§æ£€æŸ¥å™¨ âœ“")
        print("4. ReasoningTracker: æ¨ç†è·Ÿè¸ªå’Œå¯è§£é‡Šæ€§å·¥å…· âœ“")
        print("5. å‘åå…¼å®¹æ€§: ä¿æŒä¸ç°æœ‰ç³»ç»Ÿå…¼å®¹ âœ“")
        print("6. å•å…ƒæµ‹è¯•æ¡†æ¶: å®Œæ•´çš„æµ‹è¯•è¦†ç›– âœ“")
        print("7. é›†æˆéªŒè¯: ç»„ä»¶é—´ååŒå·¥ä½œæ­£å¸¸ âœ“")
    else:
        print("âŒ é›†æˆæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ã€‚")

    return all_passed


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
