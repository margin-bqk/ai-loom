# LOOM 默认配置
# 版本: 2.0.0
# 增强：支持多Provider、BYOK、性能优化

# LLM提供商配置（增强版）
llm_providers:
  # OpenAI GPT系列
  openai:
    type: openai
    enabled: true
    api_key: ${OPENAI_API_KEY:}  # 从环境变量读取（BYOK）
    base_url: https://api.openai.com/v1
    model: gpt-3.5-turbo
    temperature: 0.7
    max_tokens: 1000
    timeout: 30
    max_retries: 3
    retry_delay: 1.0
    fallback_enabled: true
    connection_pool_size: 5
    enable_batching: false
    enable_caching: true
    cache_ttl: 300  # 5分钟
    organization: ${OPENAI_ORGANIZATION:}  # 可选
  
  # Anthropic Claude系列
  anthropic:
    type: anthropic
    enabled: true
    api_key: ${ANTHROPIC_API_KEY:}  # 从环境变量读取（BYOK）
    base_url: https://api.anthropic.com/v1
    version: 2023-06-01
    model: claude-3-haiku-20240307
    temperature: 0.7
    max_tokens: 1000
    timeout: 30
    max_retries: 3
    retry_delay: 1.0
    fallback_enabled: true
    connection_pool_size: 5
    enable_batching: false
    enable_caching: true
    cache_ttl: 300
  
  # Google Gemini
  google:
    type: google
    enabled: false  # 默认禁用，需要手动启用
    api_key: ${GOOGLE_API_KEY:}  # 从环境变量读取（BYOK）
    base_url: https://generativelanguage.googleapis.com/v1beta
    model: gemini-pro
    temperature: 0.7
    max_tokens: 1000
    timeout: 30
    max_retries: 3
    retry_delay: 1.0
    fallback_enabled: true
    connection_pool_size: 5
    enable_batching: false
    enable_caching: true
    cache_ttl: 300
  
  # Azure OpenAI
  azure:
    type: azure
    enabled: false  # 默认禁用，需要手动配置
    api_key: ${AZURE_OPENAI_API_KEY:}  # 从环境变量读取（BYOK）
    base_url: ${AZURE_OPENAI_ENDPOINT:}  # 例如：https://your-resource.openai.azure.com
    api_version: 2023-12-01-preview
    deployment: ${AZURE_OPENAI_DEPLOYMENT:}  # 部署名称
    model: gpt-35-turbo  # Azure中的模型名称
    temperature: 0.7
    max_tokens: 1000
    timeout: 30
    max_retries: 3
    retry_delay: 1.0
    fallback_enabled: true
    connection_pool_size: 5
    enable_batching: false
    enable_caching: true
    cache_ttl: 300
  
  # 本地模型（Ollama）
  ollama:
    type: ollama
    enabled: true
    base_url: http://localhost:11434/api
    model: llama2
    temperature: 0.7
    max_tokens: 1000
    timeout: 60  # 本地模型可以设置更长超时
    max_retries: 2
    retry_delay: 2.0
    fallback_enabled: true
    connection_pool_size: 3
    enable_batching: true  # 本地模型支持批处理
    enable_caching: true
    cache_ttl: 600  # 10分钟
  
  # 本地模型（LM Studio）
  lm_studio:
    type: local
    enabled: false
    base_url: http://localhost:1234/v1  # LM Studio默认端口
    model: local-model
    temperature: 0.7
    max_tokens: 1000
    timeout: 120
    max_retries: 2
    retry_delay: 2.0
    fallback_enabled: true
    connection_pool_size: 2
    enable_batching: false
    enable_caching: true
    cache_ttl: 600

# Provider选择策略
provider_selection:
  default_provider: openai
  fallback_order:
    - openai
    - anthropic
    - google
    - ollama
    - azure
  
  # 基于会话类型的Provider选择
  session_type_mapping:
    creative_writing:
      preferred_provider: openai
      preferred_model: gpt-4
      fallback_to: anthropic
    
    world_building:
      preferred_provider: anthropic
      preferred_model: claude-3-sonnet
      fallback_to: openai
    
    code_generation:
      preferred_provider: ollama
      preferred_model: codellama
      fallback_to: openai
    
    quick_chat:
      preferred_provider: openai
      preferred_model: gpt-3.5-turbo
      fallback_to: ollama
  
  # 自动切换策略
  auto_switch:
    enabled: true
    health_check_interval: 60  # 秒
    max_failures_before_switch: 3
    switch_back_after_success: true
    switch_back_delay: 300  # 5分钟
  
  # 成本优化策略
  cost_optimization:
    enabled: true
    monthly_budget: 50.0  # 美元
    alert_threshold: 0.8  # 预算使用80%时告警
    auto_switch_to_cheaper: true
    token_counting: true
    
    # 成本优化策略
    optimization_strategies:
      - name: "use_cheaper_model_for_long_context"
        enabled: true
        threshold_tokens: 2000
      
      - name: "cache_frequent_queries"
        enabled: true
        ttl_minutes: 60
      
      - name: "batch_small_requests"
        enabled: false

# 记忆存储配置
memory:
  backend: sqlite
  db_path: ./data/loom_memory.db
  vector_store_enabled: false
  max_memories_per_session: 1000
  auto_summarize: true
  summarization_interval_days: 7

# 会话默认配置
session_defaults:
  default_canon_path: ./canon
  default_llm_provider: openai
  max_turns: null  # null表示无限制
  auto_save_interval: 5  # 每5回合自动保存
  intervention_allowed: true
  retcon_allowed: true

# 运行时配置
max_concurrent_turns: 3
log_level: INFO
data_dir: ./data
cache_enabled: true
cache_ttl_minutes: 60

# 插件配置
plugins:
  # 示例插件配置
  # example_plugin:
  #   enabled: false
  #   config_path: ./plugins/example/config.yaml

# 性能调优
performance:
  max_prompt_length: 8000
  max_memories_per_prompt: 10
  enable_response_caching: true
  cache_size_mb: 100

# 安全配置
security:
  allow_file_system_access: true
  max_session_duration_hours: 24
  intervention_rate_limit: 10  # 每分钟最大干预次数
  require_justification_for_retcon: true

# 监控配置
monitoring:
  enable_metrics: true
  metrics_port: 9090
  enable_tracing: false
  log_retention_days: 30