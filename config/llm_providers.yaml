# LLM提供商详细配置
# 此文件提供更详细的LLM提供商配置，可覆盖default_config.yaml中的设置

providers:
  # OpenAI GPT系列
  openai:
    display_name: "OpenAI GPT"
    models:
      - name: "gpt-4"
        description: "GPT-4 最新版本"
        max_tokens: 8192
        cost_per_1k_input: 0.03  # 美元
        cost_per_1k_output: 0.06  # 美元
      
      - name: "gpt-4-turbo"
        description: "GPT-4 Turbo"
        max_tokens: 4096
        cost_per_1k_input: 0.01
        cost_per_1k_output: 0.03
      
      - name: "gpt-3.5-turbo"
        description: "GPT-3.5 Turbo (默认)"
        max_tokens: 4096
        cost_per_1k_input: 0.0015
        cost_per_1k_output: 0.002
    
    capabilities:
      - "function_calling"
      - "json_mode"
      - "streaming"
    
    rate_limits:
      requests_per_minute: 60
      tokens_per_minute: 90000
  
  # Anthropic Claude系列
  anthropic:
    display_name: "Anthropic Claude"
    models:
      - name: "claude-3-opus-20240229"
        description: "Claude 3 Opus (最强)"
        max_tokens: 4096
        cost_per_1k_input: 0.015
        cost_per_1k_output: 0.075
      
      - name: "claude-3-sonnet-20240229"
        description: "Claude 3 Sonnet (平衡)"
        max_tokens: 4096
        cost_per_1k_input: 0.003
        cost_per_1k_output: 0.015
      
      - name: "claude-3-haiku-20240307"
        description: "Claude 3 Haiku (快速)"
        max_tokens: 4096
        cost_per_1k_input: 0.00025
        cost_per_1k_output: 0.00125
    
    capabilities:
      - "tool_use"
      - "vision"
    
    rate_limits:
      requests_per_minute: 100
      tokens_per_minute: 100000
  
  # 本地模型 (Ollama)
  ollama:
    display_name: "本地模型 (Ollama)"
    models:
      - name: "llama2"
        description: "Llama 2 7B"
        max_tokens: 4096
        cost_per_1k_input: 0.0
        cost_per_1k_output: 0.0
      
      - name: "mistral"
        description: "Mistral 7B"
        max_tokens: 8192
        cost_per_1k_input: 0.0
        cost_per_1k_output: 0.0
      
      - name: "codellama"
        description: "CodeLlama 7B"
        max_tokens: 4096
        cost_per_1k_input: 0.0
        cost_per_1k_output: 0.0
    
    capabilities:
      - "local_inference"
      - "no_internet_required"
    
    requirements:
      - "ollama installed"
      - "8GB+ RAM"
      - "模型已下载"
  
  # Google Gemini
  gemini:
    display_name: "Google Gemini"
    models:
      - name: "gemini-pro"
        description: "Gemini Pro"
        max_tokens: 32768
        cost_per_1k_input: 0.0005
        cost_per_1k_output: 0.0015
    
    capabilities:
      - "multimodal"
      - "function_calling"
    
    rate_limits:
      requests_per_minute: 60
      tokens_per_minute: 60000

# 提供商选择策略
selection_strategy:
  default: "openai"
  fallback_order:
    - "openai"
    - "anthropic"
    - "gemini"
    - "ollama"
  
  cost_optimization: true
  latency_optimization: false
  
  # 基于会话类型的模型选择
  session_type_mapping:
    creative_writing:
      preferred_provider: "openai"
      preferred_model: "gpt-4"
    
    world_building:
      preferred_provider: "anthropic"
      preferred_model: "claude-3-sonnet"
    
    code_generation:
      preferred_provider: "ollama"
      preferred_model: "codellama"
    
    quick_chat:
      preferred_provider: "openai"
      preferred_model: "gpt-3.5-turbo"

# 成本控制
cost_control:
  monthly_budget: 50.0  # 美元
  alert_threshold: 0.8  # 预算使用80%时告警
  auto_switch_to_cheaper: true
  token_counting: true
  
  # 成本优化策略
  optimization_strategies:
    - name: "use_cheaper_model_for_long_context"
      enabled: true
      threshold_tokens: 2000
    
    - name: "cache_frequent_queries"
      enabled: true
      ttl_minutes: 60
    
    - name: "batch_small_requests"
      enabled: false

# 监控和日志
monitoring:
  log_all_requests: false
  log_errors_only: true
  metrics_collection_interval: 60  # 秒
  
  # 性能指标
  track_metrics:
    - "response_time"
    - "token_usage"
    - "cost_per_session"
    - "error_rate"