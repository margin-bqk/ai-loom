llm_providers:
  openai:
    type: openai
    api_key: ''
    base_url: https://api.openai.com/v1
    model: gpt-3.5-turbo
    temperature: 0.7
    max_tokens: 1000
    timeout: 30
    max_retries: 3
    retry_delay: 1.0
    fallback_enabled: true
    enabled: true
    connection_pool_size: 5
    enable_batching: false
    enable_caching: true
    cache_ttl: 300
  anthropic:
    type: anthropic
    api_key: ''
    base_url: https://api.anthropic.com/v1
    model: claude-3-haiku-20240307
    temperature: 0.7
    max_tokens: 1000
    timeout: 30
    max_retries: 3
    retry_delay: 1.0
    fallback_enabled: true
    enabled: true
    connection_pool_size: 5
    enable_batching: false
    enable_caching: true
    cache_ttl: 300
  google:
    type: google
    api_key: ''
    base_url: https://generativelanguage.googleapis.com/v1beta
    model: gemini-pro
    temperature: 0.7
    max_tokens: 1000
    timeout: 30
    max_retries: 3
    retry_delay: 1.0
    fallback_enabled: true
    enabled: false
    connection_pool_size: 5
    enable_batching: false
    enable_caching: true
    cache_ttl: 300
  azure:
    type: azure
    api_key: ''
    base_url: ''
    model: gpt-35-turbo
    temperature: 0.7
    max_tokens: 1000
    timeout: 30
    max_retries: 3
    retry_delay: 1.0
    fallback_enabled: true
    enabled: false
    connection_pool_size: 5
    enable_batching: false
    enable_caching: true
    cache_ttl: 300
  ollama:
    type: ollama
    api_key: null
    base_url: http://localhost:11434/api
    model: llama2
    temperature: 0.7
    max_tokens: 1000
    timeout: 60
    max_retries: 2
    retry_delay: 2.0
    fallback_enabled: true
    enabled: true
    connection_pool_size: 3
    enable_batching: true
    enable_caching: true
    cache_ttl: 600
  lm_studio:
    type: local
    api_key: null
    base_url: http://localhost:1234/v1
    model: local-model
    temperature: 0.7
    max_tokens: 1000
    timeout: 120
    max_retries: 2
    retry_delay: 2.0
    fallback_enabled: true
    enabled: false
    connection_pool_size: 2
    enable_batching: false
    enable_caching: true
    cache_ttl: 600
provider_selection:
  default_provider: openai
  fallback_order:
  - openai
  - anthropic
  - google
  - ollama
  - azure
  session_type_mapping:
    creative_writing:
      preferred_provider: openai
      preferred_model: gpt-4
      fallback_to: anthropic
    world_building:
      preferred_provider: anthropic
      preferred_model: claude-3-sonnet
      fallback_to: openai
    code_generation:
      preferred_provider: ollama
      preferred_model: codellama
      fallback_to: openai
    quick_chat:
      preferred_provider: openai
      preferred_model: gpt-3.5-turbo
      fallback_to: ollama
  auto_switch:
    enabled: true
    health_check_interval: 60
    max_failures_before_switch: 3
    switch_back_after_success: true
    switch_back_delay: 300
  cost_optimization:
    enabled: true
    monthly_budget: 50.0
    alert_threshold: 0.8
    auto_switch_to_cheaper: true
    token_counting: true
    optimization_strategies:
    - name: use_cheaper_model_for_long_context
      enabled: true
      threshold_tokens: 2000
    - name: cache_frequent_queries
      enabled: true
      ttl_minutes: 60
    - name: batch_small_requests
      enabled: false
memory:
  backend: sqlite
  db_path: ./data/loom_memory.db
  vector_store_enabled: false
  max_memories_per_session: 1000
  auto_summarize: true
  summarization_interval_days: 7
session_defaults:
  default_canon_path: ./canon
  default_llm_provider: openai
  max_turns: null
  auto_save_interval: 5
  intervention_allowed: true
  retcon_allowed: true
narrative:
  enabled: true
  consistency_check_enabled: true
  continuity_check_enabled: true
  auto_summarize: true
  summarization_interval_turns: 10
  narrative_arc_tracking: true
  max_narrative_arcs: 5
  default_narrative_tone: neutral
  default_narrative_pace: normal
  consistency_threshold: 0.7
  max_continuity_issues: 5
  auto_archive: true
  archive_interval_turns: 50
  max_archive_versions: 10
  export_format: markdown
max_concurrent_turns: 3
log_level: DEBUG
data_dir: ./data
cache_enabled: true
cache_ttl_minutes: 60
performance:
  max_prompt_length: 8000
  max_memories_per_prompt: 10
  enable_response_caching: true
  cache_size_mb: 100
security:
  allow_file_system_access: true
  max_session_duration_hours: 24
  intervention_rate_limit: 10
  require_justification_for_retcon: true
monitoring:
  enable_metrics: true
  metrics_port: 9090
  enable_tracing: false
  log_retention_days: 30
plugins: null
